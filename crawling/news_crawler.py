# news_crawler.py
import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime, timedelta
import re

import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

# ---- ê²½ë¡œ ìƒìˆ˜ ----
BASE_DIR = Path(__file__).resolve().parent
DB_DIR = BASE_DIR / "db"
DEBUG_DIR = BASE_DIR / "debug"

DEBUG_DIR.mkdir(parents=True, exist_ok=True)

# ---- ì‹œì¥ ê´€ë ¨ ìƒìˆ˜ ----
KOSPI_MARKET_KEY = "J"
KOSDAQ_MARKET_KEY = "U"

N_KOSPI = 3   # ê²€ìƒ‰í•  ì½”ìŠ¤í”¼ ì¢…ëª© ê°œìˆ˜
N_KOSDAQ = 0  # ê²€ìƒ‰í•  ì½”ìŠ¤ë‹¥ ì¢…ëª© ê°œìˆ˜
N_NEWS_PER_STOCK = 3  # ê²€ìƒ‰í•  ì¢…ëª©ë‹¹ ë‰´ìŠ¤ ê°œìˆ˜

REQUEST_TIMEOUT = 10

# âœ… ë„¤ì´ë²„ ì¦ê¶Œ "ë‰´ìŠ¤Â·ê³µì‹œ" íƒ­ì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¿Œë¦¬ëŠ” iframe ì£¼ì†Œ
BASE_LIST_URL = (
    "https://finance.naver.com/item/news_news.naver"
    "?code={code}&page={page}&clusterId="
)
BASE_READ_URL = "https://finance.naver.com"  # ìƒëŒ€ ê²½ë¡œìš© ë² ì´ìŠ¤ URL


# ---------------------------------------------------
# ê³µí†µ ìœ í‹¸
# ---------------------------------------------------
def load_market_codes(path: Path) -> Dict[str, Dict[str, Any]]:
    with path.open("r", encoding="utf-8") as f:
        data = json.load(f)
    return data


def _parse_date(date_text: str) -> datetime:
    """ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜ (ì˜ˆì „ async ì½”ë“œ ì¬ì‚¬ìš©)"""
    try:
        # "2024.07.13 15:30" í˜•ì‹ íŒŒì‹±
        date_pattern = r"(\d{4})\.(\d{2})\.(\d{2})\s+(\d{2}):(\d{2})"
        match = re.search(date_pattern, date_text)

        if match:
            year, month, day, hour, minute = map(int, match.groups())
            return datetime(year, month, day, hour, minute)

        # ìƒëŒ€ ì‹œê°„ íŒŒì‹± ("1ì‹œê°„ ì „", "2ì¼ ì „" ë“±)
        if "ë¶„ ì „" in date_text:
            minutes = int(re.search(r"(\d+)ë¶„", date_text).group(1))
            return datetime.now() - timedelta(minutes=minutes)
        elif "ì‹œê°„ ì „" in date_text:
            hours = int(re.search(r"(\d+)ì‹œê°„", date_text).group(1))
            return datetime.now() - timedelta(hours=hours)
        elif "ì¼ ì „" in date_text:
            days = int(re.search(r"(\d+)ì¼", date_text).group(1))
            return datetime.now() - timedelta(days=days)

    except Exception as e:
        logger.warning(f"Failed to parse date '{date_text}': {str(e)}")

    return datetime.now()


def select_top_codes(
    market_data: Dict[str, Dict[str, Any]],
    market_key: str,
    limit: int,
) -> List[Dict[str, Any]]:
    """
    market_code.json ê¸°ì¤€ìœ¼ë¡œ íŠ¹ì • ì‹œì¥(KOSPI/KOSDAQ)ì˜ ìƒìœ„ ëª‡ ê°œ ì¢…ëª©ë§Œ ì¶”ë¦¬ê¸°
    (ì§€ê¸ˆì€ ë‹¨ìˆœíˆ ì¢…ëª©ì½”ë“œ ìˆœ ì •ë ¬ í›„ ì•ì—ì„œ limitê°œ ìë¦„)
    """
    items: List[Dict[str, Any]] = []
    for name, info in market_data.items():
        if info.get("market") == market_key:
            code = info.get("code")
            if not code:
                continue
            items.append(
                {
                    "name": name,
                    "code": code,
                    "market": market_key,
                }
            )

    items.sort(key=lambda x: x["code"])
    return items[:limit]


def build_list_url(code: str, page: int = 1) -> str:
    return BASE_LIST_URL.format(code=code, page=page)


# ---------------------------------------------------
# ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ íŒŒì‹±
# ---------------------------------------------------
def parse_news_list_from_list_page(html: str, max_items: int) -> List[Dict[str, Any]]:
    """
    /item/news_news.naver ì˜ HTMLì—ì„œ
    ë‰´ìŠ¤ ì œëª© + ìƒì„¸ URL(ë„¤ì´ë²„ ë‰´ìŠ¤ ì½ê¸° í˜ì´ì§€) + ë‚ ì§œë¥¼ ë½‘ëŠ”ë‹¤.
    """
    soup = BeautifulSoup(html, "html.parser")
    results: List[Dict[str, Any]] = []

    # ê¸°ë³¸ êµ¬ì¡°: table.type2 ì•ˆì˜ tr ë“¤
    rows = soup.select("table.type2 tr")

    for tr in rows:
        if len(results) >= max_items:
            break

        title_link = tr.select_one("td.title a")
        if not title_link:
            continue

        title = title_link.get_text(strip=True)
        href = title_link.get("href", "").strip()
        if not title or not href:
            continue

        # ìƒëŒ€/ì ˆëŒ€ URL ëª¨ë‘ ì²˜ë¦¬
        if href.startswith("http"):
            url = href
        elif href.startswith("/"):
            url = BASE_READ_URL + href
        else:
            url = BASE_READ_URL + "/" + href.lstrip("/")

        # ë‚ ì§œ(td.date) ì¶”ì¶œ ì‹œë„
        date_td = tr.select_one("td.date")
        if date_td:
            date_text = date_td.get_text(strip=True)
            published_at = _parse_date(date_text).isoformat()
        else:
            published_at = datetime.now().isoformat()

        results.append(
            {
                "title": title,
                "detail_url": url,
                "published_at": published_at,
            }
        )

    # ê·¸ë˜ë„ ì•„ë¬´ê²ƒë„ ì•ˆ ë‚˜ì™”ìœ¼ë©´, ì˜ˆì „ ë°©ì‹ìœ¼ë¡œ í•œ ë²ˆ ë” ì‹œë„
    if not results:
        links = soup.select("table.type2 td.title a")
        if not links:
            links = soup.select("td.title a")
        if not links:
            all_links = soup.find_all("a", href=True)
            links = [
                a for a in all_links
                if "item/news_read.naver" in a["href"]
            ]

        for a in links[:max_items]:
            title = a.get_text(strip=True)
            href = a.get("href", "").strip()
            if not title or not href:
                continue

            if href.startswith("http"):
                url = href
            elif href.startswith("/"):
                url = BASE_READ_URL + href
            else:
                url = BASE_READ_URL + "/" + href.lstrip("/")

            results.append(
                {
                    "title": title,
                    "detail_url": url,
                    "published_at": datetime.now().isoformat(),
                }
            )

    if not results:
        logger.warning("parse_news_list: no news items found on page")
        return results

    logger.info(f"parse_news_list: extracted {len(results)} items from list page")
    return results


# ---------------------------------------------------
# ìƒì„¸ ê¸°ì‚¬ íŒŒì‹± (ë„¤ì´ë²„ ë‰´ìŠ¤ ì¼ë°˜ í˜ì´ì§€ìš©)
# ---------------------------------------------------
def _parse_naver_news_html(html: str) -> Dict[str, str]:
    """ë„¤ì´ë²„ ë‰´ìŠ¤(í†µí•©/ëª¨ë°”ì¼ ë“±) HTMLì—ì„œ ì œëª©/ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ."""
    soup = BeautifulSoup(html, "html.parser")

    # ì œëª© í›„ë³´ë“¤
    title_el = (
        soup.select_one("h2#title_area")
        or soup.select_one("h2.media_end_head_headline")
        or soup.select_one("h3#articleTitle")          # ì˜ˆì „ ìŠ¤íƒ€ì¼
        or soup.select_one("h1#news_headline")         # ê¸°íƒ€ ì˜ˆì™¸
    )
    title = title_el.get_text(strip=True) if title_el else ""

    # ë³¸ë¬¸ í›„ë³´ë“¤
    content_el = (
        soup.select_one("article#dic_area")           # í†µí•© ë‰´ìŠ¤
        or soup.select_one("div#newsct_article")      # í†µí•© ë‰´ìŠ¤ ë‹¤ë¥¸ ì¼€ì´ìŠ¤
        or soup.select_one("div#articleBodyContents") # ì˜ˆì „ ìŠ¤íƒ€ì¼
        or soup.select_one("div.article_view")        # ì¼ë¶€ ì–¸ë¡ ì‚¬ ìì²´ í…œí”Œë¦¿
    )

    content = ""
    if content_el:
        for tag in content_el(["script", "style"]):
            tag.decompose()
        content = content_el.get_text(" ", strip=True)

    return {"title": title, "content": content}


def fetch_article_detail(session, url: str) -> dict | None:
    """
    ë„¤ì´ë²„ ê¸ˆìœµ ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
    - ê¸ˆìœµ wrapper(.articleSubject, .articleCont) ìš°ì„  ì‹œë„
    - ì‹¤íŒ¨/ë„ˆë¬´ ì§§ìœ¼ë©´ í†µí•© ë‰´ìŠ¤/ëª¨ë°”ì¼ êµ¬ì¡°(_parse_naver_news_html)ë¡œ ì¬ì‹œë„
    """
    logger.info(f"Requesting article page: {url}")
    try:
        resp = session.get(url, timeout=REQUEST_TIMEOUT)
        resp.raise_for_status()
    except Exception as e:
        logger.warning(f"Failed to fetch article page {url}: {e}")
        return None

    html = resp.text
    soup = BeautifulSoup(html, "html.parser")

    # ----------------------------
    # 1ì°¨: ë„¤ì´ë²„ ê¸ˆìœµ wrapper ìŠ¤íƒ€ì¼ (.articleSubject, .articleCont)
    # ----------------------------
    title_elem = soup.select_one(".articleSubject")
    content_elem = soup.select_one(".articleCont")

    title = title_elem.get_text(strip=True) if title_elem else ""
    content = ""
    if content_elem:
        for tag in content_elem(["script", "style"]):
            tag.decompose()
        content = content_elem.get_text(" ", strip=True)

    # ----------------------------
    # 2ì°¨: í†µí•© ë‰´ìŠ¤/ëª¨ë°”ì¼ ë“±ì˜ ê³µí†µ íŒŒì„œë¡œ ì¬ì‹œë„
    #  - news.naver.com ì˜ #dic_area, #newsct_article ë“±
    # ----------------------------
    if len(title) < 5 or len(content) < 50:
        parsed = _parse_naver_news_html(html)
        if parsed.get("title"):
            title = parsed["title"]
        if parsed.get("content"):
            content = parsed["content"]

    # ----------------------------
    # ë‚ ì§œ íŒŒì‹±
    # ----------------------------
    published_at = datetime.now()

    # ê¸ˆìœµ wrapper ìª½ ë‚ ì§œ
    date_elem = soup.select_one(".article_info .dates")
    date_text = ""
    if date_elem:
        date_text = date_elem.get_text(strip=True)

    # í†µí•© ë‰´ìŠ¤ ìª½ ë‚ ì§œ (news.naver.com)
    if not date_text:
        time_el = soup.select_one(".media_end_head_info_datestamp_time")
        if time_el:
            date_text = time_el.get_text(strip=True)

    if date_text:
        published_at = _parse_date(date_text)

    # ----------------------------
    # ìµœì¢… ìœ íš¨ì„± ì²´í¬
    # ----------------------------
    if not title or len(content) < 50:
        logger.warning(
            f"Article seems invalid (title/content too short): {url} "
            f"(title_len={len(title)}, content_len={len(content)})"
        )
        # ë””ë²„ê·¸ìš©ìœ¼ë¡œ HTML ì €ì¥í•´ì„œ ë‚˜ì¤‘ì— êµ¬ì¡° ì‚´í´ë³´ê³  ì‹¶ìœ¼ë©´ ì£¼ì„ í•´ì œ
        # try:
        #     fname = DEBUG_DIR / ("article_" + re.sub(r'[^0-9A-Za-z]+', '_', url) + ".html")
        #     fname.write_text(html, encoding="utf-8")
        #     logger.info(f"Saved debug article HTML to: {fname}")
        # except Exception as e:
        #     logger.error(f"Failed to save article HTML debug: {e}")
        return None

    return {
        "title": title,
        "content": content,
        "url": url,
        "published_at": published_at.isoformat(),
        "source": "naver_finance",
    }


# ---------------------------------------------------
# ì¢…ëª©ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§
# ---------------------------------------------------
def fetch_company_news(
    session: requests.Session,
    company: Dict[str, Any],
    max_items: int,
) -> List[Dict[str, Any]]:
    name = company["name"]
    code = company["code"]
    market = company["market"]

    logger.info(f"[{market}] {name}({code}) -> Naver Finance news")

    collected: List[Dict[str, Any]] = []
    page = 1

    # í˜ì´ì§€ë¥¼ 3í˜ì´ì§€ê¹Œì§€ë§Œ íƒìƒ‰ (ë„ˆë¬´ ë§ì´ íƒ€ì§€ ì•Šë„ë¡ ì œí•œ)
    while len(collected) < max_items and page <= 3:
        url = build_list_url(code, page=page)
        logger.info(f"Requesting list page {page}: {url}")

        try:
            resp = session.get(url, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
        except requests.RequestException as e:
            logger.error(f"Request failed for {name}({code}) page {page}: {e}")
            break

        # ğŸ” ë””ë²„ê·¸ìš© HTML ì €ì¥
        debug_path = DEBUG_DIR / f"{code}_page{page}.html"
        try:
            debug_path.write_text(resp.text, encoding="utf-8")
            logger.info(f"Saved debug HTML to: {debug_path}")
        except Exception as e:
            logger.error(f"Failed to save debug HTML for {code} page {page}: {e}")

        # ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ì œëª©+URL(+ë‚ ì§œ) ì¶”ì¶œ
        rough_list = parse_news_list_from_list_page(
            resp.text,
            max_items=max_items - len(collected),
        )

        if not rough_list:
            logger.info(f"No more articles found for {name}({code}) at page {page}")
            break

        # ê° ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì— ëŒ€í•´ ì‹¤ì œ ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚´ìš© ì¶”ì¶œ
        for item in rough_list:
            if len(collected) >= max_items:
                break

            detail_url = item["detail_url"]

            # 1ì°¨: ìƒì„¸ í˜ì´ì§€ ì‹œë„
            article = fetch_article_detail(session, detail_url)

            # 2ì°¨: ì‹¤íŒ¨í•˜ë©´ ë¦¬ìŠ¤íŠ¸ ì •ë³´ë§Œìœ¼ë¡œë¼ë„ ê¸°ì‚¬ ê°ì²´ ìƒì„±
            if not article:
                article = {
                    "title": item["title"],
                    "content": "",
                    "url": detail_url,
                    "published_at": item.get(
                        "published_at", datetime.now().isoformat()
                    ),
                    "source": "naver_finance_list",
                }

            # ê³µí†µìœ¼ë¡œ íšŒì‚¬ ì •ë³´ ë¶™ì—¬ì£¼ê¸°
            article["company_name"] = name
            article["company_code"] = code
            article["market"] = market

            collected.append(article)
            # ë„ˆë¬´ ë¹ ë¥´ê²Œ ë„ë°°í•˜ì§€ ì•Šë„ë¡ ì‚´ì§ ë”œë ˆì´
            time.sleep(0.1)

        page += 1

    logger.info(f"Collected {len(collected)} articles for {name}({code})")
    return collected


# ---------------------------------------------------
# ì „ì²´ ì‹œì¥ ëŒë¦¬ê¸° + íŒŒì¼ë¡œ ì €ì¥
# ---------------------------------------------------
def crawl_all_to_file(
    market_code_path: Path,
    output_path: Path,
    sleep_seconds: float = 0.2,
):
    logger.info(f"Loading market codes from: {market_code_path}")
    market_data = load_market_codes(market_code_path)

    kospi_list = select_top_codes(market_data, KOSPI_MARKET_KEY, N_KOSPI)
    kosdaq_list = select_top_codes(market_data, KOSDAQ_MARKET_KEY, N_KOSDAQ)
    targets = kospi_list + kosdaq_list

    logger.info(
        f"Selected {len(kospi_list)} KOSPI + {len(kosdaq_list)} KOSDAQ = "
        f"{len(targets)} total companies"
    )

    session = requests.Session()
    session.headers.update(
        {
            "User-Agent": (
                "Mozilla/5.0 (X11; Linux x86_64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/129.0.0.0 Safari/537.36"
            ),
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": "https://finance.naver.com/",
        }
    )

    crawling_data: Dict[str, List[Dict[str, Any]]] = {}

    total = len(targets)
    for idx, company in enumerate(targets, start=1):
        name = company["name"]
        logger.info(f"=== [{idx}/{total}] {name} ({company['code']}) ===")
        articles = fetch_company_news(session, company, N_NEWS_PER_STOCK)
        if articles:
            crawling_data[name] = articles

        time.sleep(sleep_seconds)

    logger.info(
        f"Crawling finished. Companies with news: {len(crawling_data)}"
    )

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(crawling_data, f, ensure_ascii=False, indent=2)

    logger.info(f"Saved crawling data to: {output_path}")


# ---------------------------------------------------
# ì‹¤í–‰ ì—”íŠ¸ë¦¬
# ---------------------------------------------------
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(name)s | %(levelname)s | %(message)s",
    )

    crawl_all_to_file(
        market_code_path=DB_DIR / "market_code.json",
        output_path=DB_DIR / "crawling.json",
        sleep_seconds=0.2,
    )
